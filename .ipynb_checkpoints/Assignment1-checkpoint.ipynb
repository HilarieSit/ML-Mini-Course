{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video series by 3Blue1Brown introduced you to the underlying mathematical concepts behind how artificial neural networks work. Let's now try to implement these calculations with Tensorflow (TF), a machine learning framework designed by Google. Tensorflow allows you to easily implement parallel computing and compute across CPUs/GPUs/TPUs, which are processing units that execute commands from your programs (we can talk about this later). Tensorflow operations closely follow NumPy operations, and many of the more verbose operations, such as one-hot encoding, are included in the library. Begin by importing NumPy and Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest form, a neural network consists of an input layer, hidden layer, and output layer. Each layer consists of a selected number of neurons (displayed as circles). Each layer is connected to the next by linear classifiers and its sum... Terminology and architecture will be discussed more in detail as we code the neural network with Tensorflow step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neural_network.jpg\">\n",
    "<center> Image source: https://www.nicolamanzini.com/single-hidden-layer-neural-network/ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs & Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the imaginary dataset given below. Haha, a civil engineering example! :) Earthquakes rated on the Richter scale are classified as minor and severe based on casualties and cost of damage. Our inputs are numerical and their associated outputs are catagorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([3, 3, 3, 4, 4, 6, 6, 6, 7, 7, 8, 8, 9, 9, 9])\n",
    "labels = np.array(['minor', 'minor', 'minor', 'minor', 'minor', 'minor', 'severe', 'minor', 'minor', 'severe', 'severe', 'severe', 'severe', 'severe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow allows us to define operations before actually computing them, which is useful if you want to run calculations at specific times or multiple times. Placeholders are TF objects that are fed with information at the time of computation (when we \"run a session\"). We need to specify their shape, which must match the shape of the NumPy arrays we feed in! \n",
    "\n",
    "Think about the shape of the inputs and outputs and construct TF placeholders for them (name input placeholder as x_train and label placeholder as y_train):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =\n",
    "y_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the discussion about one-hot encoding your catagorical variables? Tensorflow makes this task extremely simple. Make y_train one-hot encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Input to Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neuron in the input layer is connected to every neuron in the hidden layer by a linear classifier. It is easiest to visualize a linear classifier as a line, which you probably know has the general form $y = mx+b$, that spacially seperates the data points.\n",
    "\n",
    "<img src=\"linearclassifier.png\">\n",
    "\n",
    "<center> Image source: http://mlpy.sourceforge.net/docs/3.2/lin_class.html </center>\n",
    "\n",
    "In very simple terms, we want to solve for the m (we will call these the weights) and b (biases) so that when we plug in x (inputs) it is correctl.\n",
    "\n",
    "Notice that at a single node in... have multiple linear classifiers for all the neurons. ...just makes the collection of equations easier to write.\n",
    "\n",
    "Because the weights and biases are evaluated during a session, they should be defined as TF variables. Shapes must be specified, so we will need to do some math to determine the shape of the weights and biases. The hidden layer contains 10 neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \n",
    "b = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform matrix multiplication on the inputs and weights, and add bias with TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying nonlinear activations on the neurons of the hidden layer is necessary, because adding two linear classifiers in a row... The nonlinear activations kind of mimic thresholding of neurons in the brain... There are many nonlinear activations, such as sigmoid, tanh and ReLU. Apply a ReLU activation function to your previous answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Hidden Layer to Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neuron in the hidden layer is connected to every neuron in the output layer (the same way that the input layer was connected to the hidden layer). Connect the hidden layer to your output layer (you should define new names for your variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than a ReLU function, a softmax function is applied to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a loss function that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network model is now completely built! It's time to prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward feeding and Backprogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a tensorflow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Project 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool right? You just coded a simple neural network for classifying earthquakes. You now have the skills to start constructing neural networks to classify datasets of your choice. Find or construct a dataset, and try building the neural network on your own on a text editor/IDE (Atom, Brackets, Visual Studios, etc.). Depending on the dataset, you may run into difficulties, but don't worry, we'll discuss common problems next and add complexities to our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
